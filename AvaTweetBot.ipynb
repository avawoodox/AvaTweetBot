{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AvaTweetBot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFLkf4L9mbCC",
        "colab_type": "text"
      },
      "source": [
        "Import keras API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2PC-P1imYQv",
        "colab_type": "code",
        "outputId": "4cd72f96-f62d-49b8-d7fb-c459b2babda5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras import Sequential, Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Embedding, LSTM, Dense, Input, Bidirectional, RepeatVector, Concatenate, Activation, Dropout\n",
        "from keras.activations import softmax\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e90FwOjAcSq",
        "colab_type": "text"
      },
      "source": [
        "*Import twitter API & create app object*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6PuJs9J_6MH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tweepy\n",
        "consumer_key = \"JLBMyDbzgUqb5MxBUOdXRMfSM\"\n",
        "consumer_secret = \"C01WULLbjHpYWxWwisOz5CnHCFzDF5lxNSPwgYYkQPxJN0aohm\"\n",
        "access_token = \"848193493-hBvelUojIMU2LdhpDkJfRhecVmcwk1Wtqu9uLNgL\"\n",
        "access_token_secret = \"DEab2D6pfdXmzPqvSMWrIIkOROaW8wXuXpWtZ5vxTSqqf\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpvjMJgCAyJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the authentication object\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "# Setting your access token and secret\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "# Creating the API object while passing in auth information\n",
        "api = tweepy.API(auth) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OXmFvk0Aawz",
        "colab_type": "text"
      },
      "source": [
        "# Get tweets, do some preprocessing, and save"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGHJuCuyIJZ0",
        "colab_type": "text"
      },
      "source": [
        "Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyYO5cPdKgU-",
        "colab_type": "code",
        "outputId": "c2792fd0-2b5d-47ee-dff3-82ed19de899f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4KVL15xXkMM",
        "colab_type": "text"
      },
      "source": [
        "I'm not really going to do any preprocessing for now... all I have done is put \n",
        "everything in lowercase and split on spaces. I have also removed retweets. Return to this later. Numpy array saved to parsed_tweets.npy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT0mSJSkC7dj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#the number of items to read\n",
        "NUM_ITEMS = 10000\n",
        "\n",
        "\n",
        "def process_status(status):\n",
        "    #prune retweets\n",
        "    if status.retweeted:\n",
        "        return\n",
        "\n",
        "    #split on spaces, to lowercase\n",
        "    words = status.text.lower().split()\n",
        "\n",
        "    #add start and end tags\n",
        "    words = ['<START>'] + words + ['<END>']\n",
        "    \n",
        "    #for some reason some of these retweets are not tagged as retweets??\n",
        "    if words[1] == 'rt':\n",
        "        return\n",
        "    #add to tweet list\n",
        "    tweet_list.append(words)\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    tweet_list = []\n",
        "    for status in tweepy.Cursor(api.user_timeline).items(NUM_ITEMS):\n",
        "      process_status(status)\n",
        "    #save in numpy array\n",
        "    np_tweet_list = np.array(tweet_list)\n",
        "    np.save('gdrive/My Drive/tweetbot/parsed_tweets.npy', np_tweet_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqJkBHR2YGj3",
        "colab_type": "text"
      },
      "source": [
        "# Creat word indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zatRJ6qeYJ1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parsed_tweets = np.load('drive/My Drive/tweetbot/parsed_tweets.npy', allow_pickle=True)\n",
        "vocab = set()\n",
        "for tweet in parsed_tweets:\n",
        "  for word in tweet:\n",
        "    vocab.add(word)\n",
        "vocab = list(vocab)\n",
        "vocab.sort()\n",
        "id_to_word = {}\n",
        "#we are not including 0 as an index.\n",
        "for i in range(0,len(vocab)):\n",
        "  id_to_word[i+1] = vocab[i]\n",
        "\n",
        "word_to_id = {}\n",
        "for key, value in id_to_word.items():\n",
        "  word_to_id[value] = key\n",
        "assert(len(id_to_word) == len(word_to_id))\n",
        "\n",
        "VOCAB_SIZE = len(vocab)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3f4N-yScZPp",
        "colab_type": "text"
      },
      "source": [
        "Test Word Indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM3uUud2fzqK",
        "colab_type": "text"
      },
      "source": [
        "# Creating an Input Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3xV3NdKf5uL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_generator(batch_size=128):\n",
        "  #global input, output lists\n",
        "  input_list = []\n",
        "  output_list = []\n",
        "  example_count = 0\n",
        "  VOCAB_SIZE = len(word_to_id)\n",
        "  MAX_LEN = 40 #this is subject to change -- we want within 40 chars\n",
        "  while True:\n",
        "    for tweet in tweet_list:\n",
        "      tweet_ids = []\n",
        "      for word in tweet:\n",
        "        tweet_ids.append(word_to_id[word])\n",
        "      for slice_index in range(1,len(tweet)):\n",
        "        example_count +=1\n",
        "        #generate input\n",
        "        input = tweet_ids[:slice_index]\n",
        "        input = input + [0]*(MAX_LEN - len(input)) #pad with zeros\n",
        "\n",
        "        #generate output\n",
        "        output= [0]*(VOCAB_SIZE+1)\n",
        "        target_word_id = tweet_ids[slice_index]\n",
        "        if target_word_id not in range(len(output)):\n",
        "          print(target_word_id)\n",
        "          print(len(output))\n",
        "          print(VOCAB_SIZE)\n",
        "          print(len(word_to_id))\n",
        "          continue\n",
        "        output[target_word_id] = 1\n",
        "        #print(target_word_id)\n",
        "        #add to global lists\n",
        "        input_list.append(input)\n",
        "        output_list.append(output)\n",
        "        #return a batch of generated input/output\n",
        "        if example_count % batch_size == 0:\n",
        "          pair = (np.array(input_list), np.array(output_list))\n",
        "          assert len(input_list) == len(output_list)\n",
        "          assert len(input_list) == batch_size\n",
        "          yield pair\n",
        "          input_list = []\n",
        "          output_list = []\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrCnsdjFhcne",
        "colab_type": "text"
      },
      "source": [
        "test the generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwtELJUshefh",
        "colab_type": "code",
        "outputId": "28be6374-c3af-4699-aac0-96d47202c76a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "batch_size = 10\n",
        "generator = training_generator(batch_size)\n",
        "results= next(generator)\n",
        "input, output = results\n",
        "for row in input:\n",
        "  for word in row:\n",
        "    if word in id_to_word:\n",
        "      print(id_to_word[word])\n",
        "  print('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<START>\n",
            "\n",
            "\n",
            "<START>\n",
            "@nctsmtown_127\n",
            "\n",
            "\n",
            "<START>\n",
            "@nctsmtown_127\n",
            "that\n",
            "\n",
            "\n",
            "<START>\n",
            "@nctsmtown_127\n",
            "that\n",
            "hoodie\n",
            "\n",
            "\n",
            "<START>\n",
            "@nctsmtown_127\n",
            "that\n",
            "hoodie\n",
            "looks\n",
            "\n",
            "\n",
            "<START>\n",
            "@nctsmtown_127\n",
            "that\n",
            "hoodie\n",
            "looks\n",
            "so\n",
            "\n",
            "\n",
            "<START>\n",
            "@nctsmtown_127\n",
            "that\n",
            "hoodie\n",
            "looks\n",
            "so\n",
            "soft\n",
            "\n",
            "\n",
            "<START>\n",
            "\n",
            "\n",
            "<START>\n",
            "@fckyeahcharli\n",
            "\n",
            "\n",
            "<START>\n",
            "@fckyeahcharli\n",
            "already\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NF25_Xil9lW",
        "colab_type": "text"
      },
      "source": [
        "#Creating a Model!\n",
        "We will be using a bidirectional lstm with a word encoding layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wU9RZCo_mFhQ",
        "colab_type": "code",
        "outputId": "0ed9fcb1-92ae-4acf-ef72-de8793ed6fea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "MAX_LEN = 40\n",
        "EMBEDDING_DIM=300\n",
        "vocab_size = len(word_to_id) + 1\n",
        "\n",
        "# Text input\n",
        "text_input = Input(shape=(MAX_LEN,))\n",
        "embedding = Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_LEN)(text_input)\n",
        "x = Bidirectional(LSTM(128, return_sequences=False))(embedding)\n",
        "pred = Dense(vocab_size, activation='softmax')(x)\n",
        "model = Model(inputs=[text_input],outputs=pred)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 40)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 40, 300)           1231800   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 256)               439296    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4106)              1055242   \n",
            "=================================================================\n",
            "Total params: 2,726,338\n",
            "Trainable params: 2,726,338\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBzlmW8FmsJA",
        "colab_type": "text"
      },
      "source": [
        "#Training the Model\n",
        "We will use fit_generator to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic2G2MyHm4bK",
        "colab_type": "code",
        "outputId": "0ce153c5-c19c-423c-9925-ff5f3afacb70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        }
      },
      "source": [
        "#toggle this\n",
        "\n",
        "batch_size = 128\n",
        "generator = training_generator(batch_size)\n",
        "steps = len(tweet_list) * MAX_LEN // batch_size \n",
        "\n",
        "weight_filepath=\"weights.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(weight_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "callbacks=callbacks_list\n",
        "model.fit_generator(generator, steps_per_epoch=len(tweet_list) * MAX_LEN // batch_size, verbose=True, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "524/524 [==============================] - 98s 188ms/step - loss: 4.0082 - accuracy: 0.3256\n",
            "Epoch 2/10\n",
            "524/524 [==============================] - 98s 187ms/step - loss: 3.6834 - accuracy: 0.3755\n",
            "Epoch 3/10\n",
            "524/524 [==============================] - 100s 191ms/step - loss: 3.4004 - accuracy: 0.4290\n",
            "Epoch 4/10\n",
            "231/524 [============>.................] - ETA: 54s - loss: 3.1219 - accuracy: 0.4739"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-234-f49ef0778196>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mMAX_LEN\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3784\u001b[0m               'You must feed a value for placeholder %s' % (tensor,))\n\u001b[1;32m   3785\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3786\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3787\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3788\u001b[0m         \u001b[0;31m# Temporary workaround due to `convert_to_tensor` not casting floats.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1281\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    260\u001b[0m   \"\"\"\n\u001b[1;32m    261\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 262\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    268\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li_X0YWT9yBy",
        "colab_type": "text"
      },
      "source": [
        "save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFOUrV5-9Okt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(\"gdrive/My Drive/tweetbot/model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPRDZWnG94C3",
        "colab_type": "text"
      },
      "source": [
        "load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nphgaNws9z59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(\"drive/My Drive/tweetbot/model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dGSsvE_z0ij",
        "colab_type": "text"
      },
      "source": [
        "#Decoder\n",
        "Greedy decoder here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyFkxxKSz4Iy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decoder():\n",
        "  sentence = ['<START>']\n",
        "  sentence_nums = [word_to_id['<START>']]\n",
        "  end_reached = False\n",
        "  while end_reached == False:\n",
        "    input = np.array([sentence_nums+(40-len(sentence_nums))*[0.0]])\n",
        "    output = model.predict(input)\n",
        "    output = output[0]\n",
        "    max_ind = np.argmax(output, axis = 0)\n",
        "    token = id_to_word[max_ind]\n",
        "    sentence.append(token)\n",
        "    sentence_nums.append(max_ind)\n",
        "    if token == '<END>':\n",
        "      end_reached = True\n",
        "  return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDdUP9Yo-LI_",
        "colab_type": "code",
        "outputId": "7c06043f-83e5-4635-8835-63c444a5ba37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(decoder())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<START>', 'i', 'want', 'to', 'see', 'taemin', '<END>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqUIiQwY-rV8",
        "colab_type": "text"
      },
      "source": [
        "#Beam Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bqSvzb0-tNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def beam_decoder(n):\n",
        "  sequence = [word_to_id['<START>']] #sequences stored as lists of word ids\n",
        "  sequence_tuples = [(1,sequence)] #tuples of (probability, sequence)\n",
        "  end_reached = False\n",
        "  for i in range(MAX_LEN):\n",
        "    candidates = [] #will store the n*n candidates generated by this iteration\n",
        "    #iterate through n sequences\n",
        "    for sequence_tuple in sequence_tuples:\n",
        "      prob, sequence = sequence_tuple\n",
        "      input = np.array([sequence+(40-len(sequence))*[0.0]])\n",
        "      output = model.predict(input)\n",
        "      output = output[0]\n",
        "      n_largest_inds = (-output).argsort()[:n]\n",
        "      #for each sequence, generate n new candidate sequences\n",
        "      for ind in n_largest_inds:\n",
        "        cand_seq = sequence + [ind]\n",
        "        cand_prob = prob * output[ind]\n",
        "        candidate = (cand_prob, cand_seq)\n",
        "        candidates.append(candidate)\n",
        "    #prune the candidates and repeat\n",
        "    candidates.sort(reverse = True)\n",
        "    if len(candidates) >n:\n",
        "      candidates = candidates[:n]\n",
        "    sequence_tuples = candidates\n",
        "  \n",
        "  #Now that we have reached max_len, output the most likely candidate\n",
        "  max_prob, max_sequence = max(sequence_tuples)\n",
        "  sentence = []\n",
        "  for id in max_sequence:\n",
        "    sentence.append(id_to_word[id])\n",
        "    if id == word_to_id['<END>']:\n",
        "      break\n",
        "  return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrC93N_O_Kcb",
        "colab_type": "code",
        "outputId": "95dd707a-e0e3-4dc0-c559-6ad543b4d4e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "beam_decoder(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<START>', 'i', 'want', 'to', 'see', 'taemin', '<END>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx0MavA2Aovo",
        "colab_type": "text"
      },
      "source": [
        "# Sample Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMv3dqGHArL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_decoder():\n",
        "  sentence = ['<START>']\n",
        "  sentence_nums = [word_to_id['<START>']]\n",
        "  end_reached = False\n",
        "  while end_reached == False:\n",
        "    input = np.array([sentence_nums+(40-len(sentence_nums))*[0.0]])\n",
        "    output = model.predict(input)\n",
        "    output = output[0]\n",
        "    output = output/(1.00001)\n",
        "    random_word = np.random.multinomial(1, output, size=1)\n",
        "    max_ind = np.argmax(random_word[0], axis = 0)\n",
        "    token = id_to_word[max_ind]\n",
        "    sentence.append(token)\n",
        "    sentence_nums.append(max_ind)\n",
        "    if token == '<END>':\n",
        "      end_reached = True\n",
        "  return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkkbX1O3AxHs",
        "colab_type": "code",
        "outputId": "898a3904-9e44-486a-97a3-57a0b538dadc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for i in range(10): \n",
        "    print(sample_decoder())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<START>', 'didn’t', 'valley', 'ate', '<END>']\n",
            "['<START>', '@edw1ndrood', 'great', '<END>']\n",
            "['<START>', 'https://t.co/rcqljlwawd', 'https://t.co/l7ayujxjyw', '<END>']\n",
            "['<START>', 'i’m', 'can’t', 'scared', 'to', 'bridge', 'with', 'saving', 'dark:', 'robyn', 'square', 'wear', 'which', 'end', 'are', 'https://t.co/r02rjl1gm8', '<END>']\n",
            "['<START>', 'yummy', 'is', '....', '<END>']\n",
            "['<START>', 'this', 'girl', '🕯', 'literally', 'done', 'fact', 'for', 'all', 'your', 'jepsen', 'https://t.co/6ylev2gj00', '<END>']\n",
            "['<START>', 'talking', 'https://t.co/id5xo8wmbb', '<END>']\n",
            "['<START>', 'when', 'taemin', 'said', 'https://t.co/boioi5li2i', '<END>']\n",
            "['<START>', 'zoo', 'can’t', 'ate', 'https://t.co/vnlj4gxgez', '<END>']\n",
            "['<START>', 'carly', 'mood', 'with', 'right', 'such', 'good', 'omg', '<END>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSYNQWj3Caay",
        "colab_type": "text"
      },
      "source": [
        "# Semi-greedy decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnSYz5wKCdJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  def semi_beam(n):\n",
        "    #Get a random first word.\n",
        "    sentence = ['<START>']\n",
        "    sentence_nums = [word_to_id['<START>']]\n",
        "    input = np.array([sentence_nums+(40-len(sentence_nums))*[0.0]])\n",
        "    output = model.predict(input)\n",
        "    output = output[0]\n",
        "    output = output/(1.00001)\n",
        "    random_word = np.random.multinomial(1, output, size=1)\n",
        "    max_ind = np.argmax(random_word[0], axis = 0)\n",
        "    token = id_to_word[max_ind]\n",
        "    sentence.append(token)\n",
        "    sentence_nums.append(max_ind)\n",
        "    #greedily decode from there\n",
        "    return beam_decoder(n,sentence_nums)\n",
        "\n",
        "\n",
        "  def beam_decoder(n, sequence):\n",
        "    sequence_tuples = [(1,sequence)] #tuples of (probability, sequence)\n",
        "    end_reached = False\n",
        "    for i in range(1,MAX_LEN):\n",
        "      candidates = [] #will store the n*n candidates generated by this iteration\n",
        "      #iterate through n sequences\n",
        "      for sequence_tuple in sequence_tuples:\n",
        "        prob, sequence = sequence_tuple\n",
        "        input = np.array([sequence+(40-len(sequence))*[0.0]])\n",
        "        output = model.predict(input)\n",
        "        output = output[0]\n",
        "        n_largest_inds = (-output).argsort()[:n]\n",
        "        #for each sequence, generate n new candidate sequences\n",
        "        for ind in n_largest_inds:\n",
        "          cand_seq = sequence + [ind]\n",
        "          cand_prob = prob * output[ind]\n",
        "          candidate = (cand_prob, cand_seq)\n",
        "          candidates.append(candidate)\n",
        "      #prune the candidates and repeat\n",
        "      candidates.sort(reverse = True)\n",
        "      if len(candidates) >n:\n",
        "        candidates = candidates[:n]\n",
        "      sequence_tuples = candidates\n",
        "    \n",
        "    #Now that we have reached max_len, output the most likely candidate\n",
        "    max_prob, max_sequence = max(sequence_tuples)\n",
        "    sentence = []\n",
        "    for id in max_sequence:\n",
        "      sentence.append(id_to_word[id])\n",
        "      if id == word_to_id['<END>']:\n",
        "        break\n",
        "    return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aByuLiSNjVFl",
        "colab_type": "code",
        "outputId": "e00dc4de-936d-41fd-884a-459754d60992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "if __name__==\"__main__\":\n",
        "  predictions = set()\n",
        "  for i in range(50):\n",
        "    #print('.')\n",
        "    output = semi_beam(3)\n",
        "    output = output[1:]\n",
        "    output = output[:-1]\n",
        "    #print(output)\n",
        "    prediction = ''\n",
        "    for word in output:\n",
        "      prediction += word + ' '\n",
        "    #print(prediction)\n",
        "    predictions.add(prediction)\n",
        "\n",
        "\n",
        "  for prediction in predictions:\n",
        "    print(prediction)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "“i want to see taemin \n",
            "robyn was the best video of pop in time \n",
            "@ncutigatwa this is my best song on the life \n",
            "https://t.co/uwzpy3eyzg \n",
            "we love a stan \n",
            "@edw1ndrood we talk about sure \n",
            "dressed up as as halloween as it lmao \n",
            "still thinking about this video lol \n",
            "@spencergbruce i’m jealous \n",
            "i want to see taemin \n",
            "on going to see taemin \n",
            "literally hate my life \n",
            "i’m going to go see charli xcx concert \n",
            "tossed omg that my life \n",
            "phone is my life \n",
            "thinking about taemin ... also at the charli xcx concert concert \n",
            "https://t.co/qlcltq2wio \n",
            "bay need to see taemin \n",
            "stop need to see taemin \n",
            "white thinking about taemin \n",
            "won’t taemin said taemin \n",
            "professor: @spencergbruce i was just to see robyn \n",
            "@charli_xcx robyn \n",
            "@crispiesthand hate my life \n",
            "#1 was a stan \n",
            "we’re need to see the charli xcx concert on time but it 💀 \n",
            "mitchell. ate omg omg \n",
            "@arca1000000 ate omg \n",
            "😍 hate \n",
            "@musicnewsfact robyn ... taste \n",
            "https://t.co/ovt9zef21x yes \n",
            "hello need to see taemin \n",
            "2 xcx is my life \n",
            "if you need to see taemin \n",
            "u yes \n",
            "taste! yes \n",
            "remember when i want to see the robyn concert concert \n",
            "broke ate \n",
            "please hate my life \n",
            "tastes not a stan but i’m robyn tho lol \n",
            "christmas thinking about robyn ... fuck tho \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUP-ShVJDd6a",
        "colab_type": "code",
        "outputId": "c547b94a-da66-483a-d794-93fd3d9d8d7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for i in range(10):\n",
        "  print(semi_beam(3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<START>', 'joni', 'putting', 'on', 'my', 'life', '<END>']\n",
            "['<START>', 'the', 'way', 'that', 'does', 'not', 'have', 'to', 'go', 'to', 'go', 'to', 'go', 'only', 'go', 'me', '<END>']\n",
            "['<START>', 'twitter', 'is', 'a', 'good', '<END>']\n",
            "['<START>', '@spongebobbway', 'yes', 'i', 'love', 'my', 'life', '<END>']\n",
            "['<START>', 'what’s', 'is', 'a', 'one', 'on', 'my', 'life', '<END>']\n",
            "['<START>', '@agcook404…', 'ate', 'omg', '<END>']\n",
            "['<START>', 'people:', 'hate', 'my', 'life', '<END>']\n",
            "['<START>', 'give', 'me', 'in', 'the', 'robyn', 'concert', 'concert', '<END>']\n",
            "['<START>', 'let', 'do', 'me', '<END>']\n",
            "['<START>', 'omg', 'yes', '<END>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWyD1JuKPG1P",
        "colab_type": "text"
      },
      "source": [
        "Some of my test results:\n",
        "Semi-beam with lstm length 256 trained for 5 epochs with accuracy = 21%\n",
        "\n",
        "TODO: try making the model more compicated eg\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(LSTM(128))\n",
        "model2.add(Dropout(0.2))\n",
        "\n"
      ]
    }
  ]
}